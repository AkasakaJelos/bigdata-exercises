{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers – Exercises</center>\n",
    "## <center>Spring 2025 – Week 8 – ETH Zurich</center>\n",
    "## <center>Spark - Solutions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation for the exercise in Spark\n",
    "\n",
    "1. Drag this notebook and everything else in the exercise08 folder in the `notebooks` folder of your exam magic box\n",
    "\n",
    "2. Start docker with ```docker-compose up -d```\n",
    "\n",
    "3. Launch Jupiter from the docker container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is YARN?\n",
    "\n",
    "Fundamentally, “**Y**et **A**nother **R**esource **N**egotiator”. **YARN**  is a resource scheduler designed to work on existing and new Hadoop clusters. \n",
    "\n",
    "YARN supports pluggable schedulers. The task of the scheduler is to share the resources of a large cluster among different tenants (applications) while trying to meet application demands (memory, CPU). A user may have several applications active at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 List at least 3 main shortcomings of MapReduce v1 that are addressed by YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 State which of the following statements are true:\n",
    "\n",
    "1. The ResourceManager has to provide fault tolerance for resources across the cluster \n",
    "\n",
    "2. Container allocation/deallocation can take place in a dynamic fashion as the application progresses. \n",
    "\n",
    "3. YARN plans to allow applications to only request resources in terms of memory usage and number of CPUs.\n",
    "\n",
    "4. Communications between the ResourceManager and NodeManagers are heartbeat-based. \n",
    "\n",
    "5. The ResourceManager does not have a global view of all usage of cluster resources. Therefore, it tries to make better scheduling decisions based on probabilistic prediction. \n",
    "\n",
    "6. ResourceManager has the ability to request resources back from a running application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Whose responsibility is it? Say which component of YARN is resposible for each of the following tasks.\n",
    "\n",
    "1. Fault Tolerance of running applications *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "1. Asking for resources needed for an application *[ResourceManager | ApplicationMaster | NodeManager ]*\n",
    "\n",
    "1. Providing leases to use containers *[ResourceManager | ApplicationMaster | NodeManager]*\n",
    "\n",
    "1. Tracking status and progress of running applications *[ResourceManager | ApplicationMaster | NodeManager]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 What is the typical configuration for YARN? Choose for the following components how many instances of them there are in a cluster.\n",
    "\n",
    "```\n",
    "1. ResourceManager                  a. One per cluster\n",
    "\n",
    "2. ApplicationMaster                b. One per node\n",
    "\n",
    "3. NodeManager                      c. Many per cluster, but usually not per node\n",
    "\n",
    "4. Container                        d. Many per node \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apache Spark Architecuture\n",
    "\n",
    "Spark is a fast and general-purpose cluster computing platform. It builds upon the MapReduce model and extends it to efficiently support a broader range of workloads, including interactive queries and stream processing—tasks that traditionally required separate distributed systems.\n",
    "\n",
    "At a high level, every Spark application consists of a **driver program** that launches various parallel operations on a cluster. The driver program contains the application's main function and defines distributed datasets on the cluster, then applies operations to them.\n",
    "\n",
    "Driver programs access Spark through a **SparkContext** object, which represents a connection to a computing cluster. There is no need to create a SparkContext; it is created for you automatically when you run the first code cell in the Jupyter.\n",
    "\n",
    "The driver communicates with a potentially large number of distributed workers called **executors**. The driver runs in its own process and each executor is a separate process. A driver and its executors are together termed a Spark application.\n",
    "\n",
    "![Image of Account](http://spark.apache.org/docs/latest/img/cluster-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Understand Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \n",
    "\n",
    "#### What are RDD operations?\n",
    "RDDs offer two types of operations: **transformations** and **actions**.\n",
    "\n",
    "* **Transformations** create a new dataset (still RDD) from an existing one. Transformations are lazy, meaning that no transformation is executed until you execute an action.\n",
    "* **Actions** compute a result (maybe a single value) based on an RDD, and either return it to the driver program or save it to an external storage system (e.g., HDFS). Actions trigger the execution of all requested transformations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations and actions are different because of the way Spark computes RDDs. Although you can define new RDDs any time, Spark computes them only in a **lazy** fashion, that is, the first time they are used in an action. We are going to talk more about this in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. In this exercise, we will run the cells below to create RDDs from local files. Generally it is possible to read data from other resources using the following tokens:\n",
    "\n",
    "* `file`: Read from the local file system.\n",
    "* `hdfs`: Read from a Hadoop Distributed File System.\n",
    "* `s3`  : Read from AWS S3 Storage.\n",
    "* `wasb`: Read from Azure Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "# sc is the Spark Context object \n",
    "sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T17:48:55.280470Z",
     "start_time": "2021-04-30T17:48:54.387363Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2284.843017578125,
      "end_time": 1619992986444.119
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fruits = sc.textFile('./fruits.txt')\n",
    "yellowThings = sc.textFile('./yellowthings.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 RDD transformations\n",
    "Following are examples of some of the common transformations available. For a detailed list, see [RDD Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations).\n",
    "\n",
    "Run some transformations below to understand this better.\n",
    "\n",
    "**Note1:** the `collect` command is NOT a Transformation, it is an Action used here for the purposes of showing the results! \\\n",
    "**Note2:** If some of the queries are taking too long to complete, try restarting the kernel, and rerunning the cell *above*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T16:36:47.273406Z",
     "start_time": "2021-04-30T16:36:46.938589Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 13372.175048828125,
      "end_time": 1619992999827.243
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])\n",
    "fruitsReversed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T16:35:24.992136Z",
     "start_time": "2021-04-30T16:35:24.925647Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 5291.009033203125,
      "end_time": 1619993009753.202
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 755.614013671875,
      "end_time": 1619993010926.142
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 762.670166015625,
      "end_time": 1619993014486.587
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 803.803955078125,
      "end_time": 1619993016289.369
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intersection\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 758.760986328125,
      "end_time": 1619993023087.109
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 760.218017578125,
      "end_time": 1619993044556.984
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "yellowThingsByFirstLetter = yellowThings.map(lambda thing: (thing[0], thing)).groupByKey()\n",
    "for letter, lst in yellowThingsByFirstLetter.collect():\n",
    "    print(\"For letter\", letter)\n",
    "    for obj in lst:\n",
    "        print(\" > \", obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 757.301025390625,
      "end_time": 1619993047373.34
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "numFruitsByLength.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 250.887939453125,
      "end_time": 1619993067760.489
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "print(fruitsArray)\n",
    "print(yellowThingsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.635986328125,
      "end_time": 1619993079227.007
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "numFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.071044921875,
      "end_time": 1619993081636.856
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "first3Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 248.026123046875,
      "end_time": 1619993082497.212
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "# Note1: the set() operation converts each string into a set of unique characters.\n",
    "# Note2: The union used here is a set union, meaning it combines the elements of two sets and removes duplicates automatically.\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Lazy evaluation\n",
    "Lazy evaluation means that when we call a transformation on an RDD (for instance, calling `map()`), the operation is not immediately performed. Instead, Spark internally records metadata to indicate that this operation has been requested. Rather than thinking of an RDD as containing specific data, it is best to think of each RDD as\n",
    "consisting of instructions on how to compute the data that we build up through transformations. Loading data into an RDD is lazily evaluated in the same way transformations are. So, when we call `sc.textFile()`, the data is not loaded until it is necessary. As with transformations, the operation (in this case, reading the data) can\n",
    "occur multiple times.\n",
    "\n",
    "\n",
    "Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph. For instance, the code bellow corresponds to the following graph:\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/TH3rEYtgRDByKqE/download/Stage5.png\" width=\"400\">\n",
    "\n",
    "**Remember that in order to obtain data from the node on the lineage graph (our RDD of interest) you have to call an action!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.416015625,
      "end_time": 1619993087737.5
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "apples = fruits.filter(lambda x: \"apple\" in x)\n",
    "lemons = yellowThings.filter(lambda x: \"lemon\" in x)\n",
    "applesAndLemons = apples.union(lemons)\n",
    "print(applesAndLemons.collect())\n",
    "# We can also print the lineage graph\n",
    "print(applesAndLemons.toDebugString().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Persistence (Caching)\n",
    "\n",
    "Spark's RDDs are by default recomputed each time you run an action on\n",
    "them. If you would like to reuse an RDD in multiple actions, you can ask Spark to persist it using `RDD.persist()`. After computing it the first time, Spark will store the RDD contents in memory (partitioned across the machines in your cluster), and reuse them in future actions. Persisting RDDs on disk instead of memory is also possible.\n",
    "\n",
    "If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy. For the memory-only storage levels, it will recompute these partitions the next time they are accessed,\n",
    "while for the memory-and-disk ones, it will write them out to disk. In either case, this means that you don't have to worry about your job breaking if you ask Spark to cache too much data. However, caching unnecessary data can lead to eviction of useful data\n",
    "and more recomputation time. Finally, RDDs come with a method called `unpersist()` that lets you manually remove them from the cache.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To give a simple example to motivate the use of caching, let us consider the following case. Assume we have some a sample of points in our RDD and we want to compute different statistics - mean, minimum and maximum values. However, we also want to first prepare our data by doing some heavy preprocessing. If we do not use caching, we would have to recompute the preprocessing stage for each statistic, while by caching the preprocessed RDD will make us preprocess our data only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.persist()\n",
    "numFruits = fruits.count()\n",
    "first3Fruits = fruits.take(3)\n",
    "fruits.unpersist()\n",
    "print(numFruits,first3Fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Working with Key/Value Pairs\n",
    "\n",
    "\n",
    "Spark provides special operations on RDDs containing key/value pairs. These RDDs\n",
    "are called *pair RDDs*. Pair RDDs are a useful building block in many programs, as\n",
    "they expose operations that allow you to act on each key in parallel or regroup data\n",
    "across the network. For example, pair RDDs have a `reduceByKey()` method that can\n",
    "aggregate data separately for each key, and a `join()` method that can merge two\n",
    "RDDs together by grouping elements with the same key. Pair RDDs are also still RDDs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 759.595947265625,
      "end_time": 1619993113263.276
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"key1\", 0), (\"key2\", 3), (\"key1\", 8), (\"key3\", 3), (\"key3\", 9)])\n",
    "rdd2 = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Some practice\n",
    "1. What does the above code do?\n",
    "\n",
    "2. Consider the following RDD ```sales_rdd``` and write a program to compute the quantity sold for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_rdd = sc.parallelize([\n",
    "    (\"productA\", 5),\n",
    "    (\"productB\", 3),\n",
    "    (\"productA\", 2),\n",
    "    (\"productC\", 7),\n",
    "    (\"productB\", 1),\n",
    "    (\"productC\", 4),\n",
    "    (\"productC\", 2),\n",
    "    (\"productD\", 12),\n",
    "    (\"productA\", 1),\n",
    "    (\"productB\", 8),\n",
    "    (\"productB\", 9)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Converting a user program into tasks\n",
    "\n",
    "A Spark driver is responsible for converting a user program into units of physical execution called tasks. At a high level, all Spark programs follow the same structure: they create RDDs from some input, derive new RDDs from those using transformations, and perform actions to collect or save data. A Spark program implicitly creates a logical **directed acyclic graph (DAG)** of operations.\n",
    "When the driver runs, it converts this logical graph into a physical execution plan.\n",
    "\n",
    "Spark performs several optimizations, such as \"pipelining\" map transformations together to merge them, and converts the execution graph into a set of **stages**.\n",
    "Each stage, in turn, consists of multiple tasks. The tasks are bundled up and prepared to be sent to the cluster. Tasks are the smallest unit of work in Spark; a typical user program can launch hundreds or thousands of individual tasks.\n",
    "\n",
    "Each RDD maintains a pointer to one or more parents along with metadata about what\n",
    "type of relationship they have. For instance, when you call `val b = a.map()` on an\n",
    "RDD, the RDD `b` keeps a reference to its parent `a`. These pointers allow an RDD to be\n",
    "traced to all of its ancestors.\n",
    "\n",
    "The following phases occur during Spark execution:\n",
    "* User code defines a DAG (directed acyclic graph) of RDDs. Operations on RDDs create new RDDs that refer back to their parents, thereby creating a graph.\n",
    "* Actions force translation of the DAG to an execution plan. When you call an action on an RDD, it must be computed. This requires computing its parent RDDs as well. \n",
    "* Spark's scheduler submits a job to compute all needed RDDs. That job will have one or more stages, which are parallel waves of computation composed of tasks. Each stage will correspond to one or more RDDs in the DAG. A single stage can correspond to multiple RDDs due to pipelining.\n",
    "* Tasks are scheduled and executed on a cluster\n",
    "* Stages are processed in order, with individual tasks launching to compute segments of the RDD. Once the final stage is finished in a job, the action is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 44.55712890625,
      "end_time": 1557044256642.663
     }
    }
   },
   "source": [
    "## 3. The Great Language Game\n",
    "\n",
    "We will now explore the dataset that is going to be used in the graded exercise of this week. Follow the instructions below to build ```confusion-part.json```.\n",
    "\n",
    "1. Move to the `notebooks` folder in the terminal\n",
    "2. Download the data: <br>\n",
    "   ```wget https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2``` <br>\n",
    "   __or__ <br>\n",
    "   ```curl -O https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2```\n",
    "3. Extract the data: <br>\n",
    "   ```tar -jxvf confusion-2014-03-02.tbz2```\n",
    "4. Change directory to ```confusion-2014-03-02```\n",
    "5. Extract the part of the dataset that we will work with in this exercise: <br>\n",
    "   ```head -n 3000000 confusion-2014-03-02.json > confusion-part.json```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:07:34.367427Z",
     "start_time": "2021-05-03T17:07:33.474055Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 747.39990234375,
      "end_time": 1619999839916.484
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"./confusion-2014-03-02/confusion-part.json\"\n",
    "dataset = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entries are JSON records, you will need to parse them and use their respective object representations. You can use this mapping for all queries. Since some of the queries take a long time to execute on the dataset, you may want to answer these queries on the first `100000` entries. \n",
    "\n",
    "**For the rest of the exercise, fill in the results by running the queries on the created subset instead of the entire dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:11:24.581153Z",
     "start_time": "2021-05-03T17:11:23.900172Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 5296.226806640625,
      "end_time": 1619999845222.09
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "entries = dataset.map(json.loads).cache()\n",
    "print(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it. Is it working? You probably have noticed that we are just declaring RDDs without evaluating them. Now let's evaluate it using the 'take' action and look at the json objects. We are goint to look at one of the entries for which the target language is German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:31:32.131861Z",
     "start_time": "2021-05-03T15:31:31.959140Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 234.972900390625,
      "end_time": 1619999845467.318
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_german = entries.filter(lambda e: e[\"target\"] == \"German\").take(1)\n",
    "# We use json.dumps() to turn the entry into a nicely formatted string\n",
    "print(json.dumps(target_german, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's get to work. A few last things:\n",
    "- Take into account that some of the queries might have very large outputs, which Jupyter (or sometimes even Spark) won't be able to handle. It is normal for the queries to take some time, but if the notebook crashes or stops responding, try restarting the kernel. Avoid printing large outputs. You can print the first few entries to confirm the query has worked, as shown in query 1 with ```take(n)```.\n",
    "- Refer to the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html), as well as the programming guides on actions and transformations linked to above.\n",
    "\n",
    "And now to the actual queries: *Please make sure that in your queries you *only* use PySpark, and avoid any dataframes (they will covered in next week's exercises)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 How many games have been played by germans (country = \"DE\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find the number of games such that the target language is German and such that the country is Germany (DE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Find the percentage of games where Germans got to hear their native language. Round your answer to an integer. For example: 15, NOT 0.146, 14.6, 14, 14%, 15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Find games where Swiss players (country = CH) got to hear one of the official Swiss languages (target is in `official_swiss_languages`). Group the number of games by language and return the language with the highest number of games.\n",
    "\n",
    "**Hint**: use `countByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "official_swiss_languages = [\"German\", \"French\", \"Italian\", \"Romansh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 For each country, compute the percentage of games with more than 3 choices (i.e. *len(choices)* > *3*) relative to all games for that country, and display the pairs `(country, percentage)` in the descending order of the percentage. What is the second country in this list? \n",
    "\n",
    "**Hint**: use `groupByKey` to aggregate per country and `mapValues` to compute the percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = test_entries.map(lambda e: (e[\"country\"], len(e[\"choices\"]) > 3)).groupByKey()\n",
    "outcomes = correct.mapValues(lambda e: float(list(e).count(True))/len(e)).sortBy(lambda e: e[1], ascending=False).collect()\n",
    "print(outcomes[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Some theory\n",
    "\n",
    "1. Why is Spark faster than Hadoop MapReduce?\n",
    "\n",
    "2. Which of the graphs below are DAGs?\n",
    "    \n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/P4GMvUmQDLjFTRK/download\" style=\"width: 700px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. True or False\n",
    "Say if the following statements are *true* or *false*, and explain why.\n",
    "\n",
    "1. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster.\n",
    "1. Transformations construct a new RDD from a previous one and immediately calculate the result\n",
    "1. Spark's RDDs are by default recomputed each time you run an action on them\n",
    "1. After computing an RDD, Spark will automatically store its contents in memory and reuse them in future actions.\n",
    "1. When you derive new RDDs using transformations, Spark keeps track of the set of dependencies between different RDDs."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

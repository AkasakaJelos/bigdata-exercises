{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data for Engineers – Exercises</center>\n",
    "## <center>Spring 2025 – Week 8 – ETH Zurich</center>\n",
    "## <center>Notebook for the Spark Moodle Quiz</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start docker\n",
    "\n",
    "1. Drag this notebook (and everything else in the exercise08 folder) in the `notebooks` folder of your exam magic box\n",
    "\n",
    "2. Start docker with ```docker-compose up -d```\n",
    "\n",
    "3. Launch Jupiter from the docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "# sc is the Spark Context object \n",
    "sc = SparkContext('local', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 44.55712890625,
      "end_time": 1557044256642.663
     }
    }
   },
   "source": [
    "## The Moodle Quiz\n",
    "\n",
    "For this quiz we will be using the [language confusion dataset](https://lars.yencken.org/datasets/great-language-game).\n",
    "\n",
    "\n",
    "For the Moodle quiz we ask you to submit the following things:\n",
    "- The query you wrote (although it is not graded, having your query is helpful for arguing about points)\n",
    "- Something related to its output (**the only part that is graded**)\n",
    "\n",
    "You don't need to submit this notebook, only the queries you wrote.\n",
    "\n",
    "You will need the same language game dataset used in the exercises. If you still do not have created the `confusion-part.json` file, follow these instruction.\n",
    "\n",
    "1. Move to the `notebooks` folder in the terminal\n",
    "2. Download the data: <br>\n",
    "   ```wget https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2``` <br>\n",
    "   __or__ <br>\n",
    "   ```curl -O https://f003.backblazeb2.com/file/larsyencken-eu-public/greatlanguagegame/confusion-2014-03-02.tbz2```\n",
    "3. Extract the data: <br>\n",
    "   ```tar -jxvf confusion-2014-03-02.tbz2```\n",
    "4. Change directory to ```confusion-2014-03-02```\n",
    "5. Extract the part of the dataset that we will work with in this exercise: <br>\n",
    "   ```head -n 3000000 confusion-2014-03-02.json > confusion-part.json```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:07:34.367427Z",
     "start_time": "2021-05-03T17:07:33.474055Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 747.39990234375,
      "end_time": 1619999839916.484
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"./confusion-2014-03-02/confusion-part.json\"\n",
    "dataset = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entries are JSON records, you will need to parse them and use their respective object representations. You can use this mapping for all queries.\n",
    "\n",
    "**For the quiz, fill in the results by running the queries on the ```confusion-part.json``` subset instead of the entire dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T17:11:24.581153Z",
     "start_time": "2021-05-03T17:11:23.900172Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 5296.226806640625,
      "end_time": 1619999845222.09
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "entries = dataset.map(json.loads).cache()\n",
    "entries.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's get to work. A few last things:\n",
    "- Take into account that some of the queries might have very large outputs, which Jupyter (or sometimes even Spark) won't be able to handle. It is normal for the queries to take some time, but if the notebook crashes or stops responding, try restarting the kernel. Avoid printing large outputs. You can print the first few entries to confirm the query has worked, as shown in the query above.\n",
    "- Refer to the [documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html) whenever in doubt.\n",
    "\n",
    "And now to the actual queries: *Please make sure that in your queries you *only* use PySpark, and avoid any dataframes (they will covered in next week's exercises)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How many distinct (guess, target) pairs are there in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:31:40.763220Z",
     "start_time": "2021-05-03T15:31:40.039089Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2256.77880859375,
      "end_time": 1619999854080.019
     }
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Return the sample ID of the most recent game in Switzerlan (CH) where the guessed language is incorrect and the target language is German. If multiple games happened on that date, return the one with the smallest sample id.\n",
    "**Hint**: You may want to have a look at the documentation for [sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:34:36.715804Z",
     "start_time": "2021-05-03T15:34:35.945578Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 1249.218017578125,
      "end_time": 1619999878828.007
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How many countries have hosted at least one game where \"German\" was both the guessed and the correct language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:35:23.809109Z",
     "start_time": "2021-05-03T15:35:21.827970Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2255.52197265625,
      "end_time": 1619999884156.929
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find the percentage of times each country guesses correctly. What is the third most correct country? Please only return the two letter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T15:40:43.495660Z",
     "start_time": "2021-05-03T15:40:42.877495Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 744.037109375,
      "end_time": 1619999964374.737
     }
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
